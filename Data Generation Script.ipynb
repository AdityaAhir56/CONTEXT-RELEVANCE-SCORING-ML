{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "743a22f8-e3b1-47ca-9951-bb90113adf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to context_relevance_dataset.zip\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "Faker.seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "n_rows = 100000\n",
    "n_sessions = 50000\n",
    "n_users = 10000\n",
    "n_contexts = 50000\n",
    "n_authors = 5000\n",
    "\n",
    "# Categorical options\n",
    "user_roles = ['Employee', 'Manager', 'Executive', 'Admin', 'Guest']\n",
    "departments = ['HR', 'IT', 'Legal', 'Engineering', 'Sales', 'Marketing']\n",
    "document_types = ['FAQ', 'Policy', 'Report', 'Email', 'Wiki Page', 'Meeting Notes']\n",
    "languages = ['English', 'Spanish', 'French']\n",
    "\n",
    "# Function to generate realistic text\n",
    "def generate_text(min_words, max_words):\n",
    "    words = fake.words(nb=random.randint(min_words, max_words))\n",
    "    return ' '.join(words).capitalize() + '.'\n",
    "\n",
    "# Generate query/session data\n",
    "query_ids = np.arange(1, n_rows + 1)\n",
    "session_ids = np.random.randint(1, n_sessions + 1, n_rows)\n",
    "user_ids = np.random.randint(1, n_users + 1, n_rows)\n",
    "user_roles_col = np.random.choice(user_roles, n_rows)\n",
    "user_departments = np.random.choice(departments, n_rows)\n",
    "user_tenure_years = np.random.randint(0, 21, n_rows)\n",
    "\n",
    "# Queries\n",
    "queries = [\n",
    "    fake.sentence(nb_words=random.randint(4, 10)).rstrip('.') + '?'\n",
    "    for _ in range(n_rows)\n",
    "]\n",
    "query_lengths = [len(q.split()) for q in queries]\n",
    "query_timestamps = [\n",
    "    fake.date_time_between(start_date='-5y', end_date='now').strftime('%Y-%m-%d %H:%M:%S')\n",
    "    for _ in range(n_rows)\n",
    "]\n",
    "\n",
    "# Previous query in session\n",
    "prev_queries = [''] * n_rows\n",
    "session_to_queries = {}\n",
    "for i, sid in enumerate(session_ids):\n",
    "    if sid not in session_to_queries:\n",
    "        session_to_queries[sid] = i\n",
    "    else:\n",
    "        prev_queries[i] = queries[session_to_queries[sid]]\n",
    "        session_to_queries[sid] = i\n",
    "\n",
    "# Context data\n",
    "context_ids = np.random.randint(1, n_contexts + 1, n_rows)\n",
    "context_texts = [generate_text(50, 200) for _ in range(n_rows)]\n",
    "context_lengths = [len(ct.split()) for ct in context_texts]\n",
    "document_types_col = np.random.choice(document_types, n_rows)\n",
    "document_departments = np.random.choice(departments, n_rows)\n",
    "creation_dates = [fake.date_between(start_date='-15y', end_date='today').strftime('%Y-%m-%d') for _ in range(n_rows)]\n",
    "last_updated_dates = [\n",
    "    (datetime.strptime(cd, '%Y-%m-%d') + timedelta(days=random.randint(0, 2000))).strftime('%Y-%m-%d')\n",
    "    for cd in creation_dates\n",
    "]\n",
    "author_ids = np.random.randint(1, n_authors + 1, n_rows)\n",
    "author_roles = np.random.choice(user_roles, n_rows)\n",
    "version_numbers = np.random.randint(1, 11, n_rows)\n",
    "tags_list = [\n",
    "    ','.join(fake.words(nb=random.randint(3, 5)))\n",
    "    for _ in range(n_rows)\n",
    "]\n",
    "languages_col = np.random.choice(languages, n_rows, p=[0.9, 0.05, 0.05])\n",
    "\n",
    "# Numeric features\n",
    "cosine_similarity = np.round(np.random.uniform(0, 1, n_rows), 3)\n",
    "tfidf_score = np.round(np.random.uniform(0, 1, n_rows), 3)\n",
    "keyword_overlap_count = np.random.randint(0, 21, n_rows)\n",
    "bert_similarity = np.round(np.random.uniform(0, 1, n_rows), 3)\n",
    "readability_score = np.round(np.random.uniform(0, 100, n_rows), 2)\n",
    "view_count = np.random.randint(0, 1001, n_rows)\n",
    "edit_count = np.random.randint(0, 51, n_rows)\n",
    "click_count = np.random.randint(0, 101, n_rows)\n",
    "historical_relevance_avg = np.round(np.random.uniform(0, 1, n_rows), 3)\n",
    "is_multi_turn = (np.array(prev_queries) != '').astype(int)\n",
    "noise_level = np.round(np.random.uniform(0, 1, n_rows), 3)\n",
    "domain_specific_score = (user_departments == document_departments).astype(int) * np.round(np.random.uniform(0.7, 1, n_rows), 3)\n",
    "\n",
    "# Relevance score\n",
    "relevance_score = (\n",
    "    0.4 * cosine_similarity +\n",
    "    0.3 * tfidf_score +\n",
    "    0.1 * (keyword_overlap_count / 20) +\n",
    "    0.1 * bert_similarity -\n",
    "    0.1 * noise_level +\n",
    "    0.05 * domain_specific_score +\n",
    "    0.05 * historical_relevance_avg\n",
    ")\n",
    "relevance_score = np.clip(relevance_score, 0, 1)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"query_id\": query_ids,\n",
    "    \"session_id\": session_ids,\n",
    "    \"user_id\": user_ids,\n",
    "    \"user_role\": user_roles_col,\n",
    "    \"user_department\": user_departments,\n",
    "    \"user_tenure_years\": user_tenure_years,\n",
    "    \"query_text\": queries,\n",
    "    \"query_length\": query_lengths,\n",
    "    \"query_timestamp\": query_timestamps,\n",
    "    \"previous_query_text\": prev_queries,\n",
    "    \"context_id\": context_ids,\n",
    "    \"context_text\": context_texts,\n",
    "    \"context_length\": context_lengths,\n",
    "    \"document_type\": document_types_col,\n",
    "    \"document_department\": document_departments,\n",
    "    \"creation_date\": creation_dates,\n",
    "    \"last_updated_date\": last_updated_dates,\n",
    "    \"author_id\": author_ids,\n",
    "    \"author_role\": author_roles,\n",
    "    \"version_number\": version_numbers,\n",
    "    \"tags\": tags_list,\n",
    "    \"language\": languages_col,\n",
    "    \"cosine_similarity\": cosine_similarity,\n",
    "    \"tfidf_score\": tfidf_score,\n",
    "    \"keyword_overlap_count\": keyword_overlap_count,\n",
    "    \"bert_similarity\": bert_similarity,\n",
    "    \"readability_score\": readability_score,\n",
    "    \"view_count\": view_count,\n",
    "    \"edit_count\": edit_count,\n",
    "    \"click_count\": click_count,\n",
    "    \"historical_relevance_avg\": historical_relevance_avg,\n",
    "    \"is_multi_turn\": is_multi_turn,\n",
    "    \"noise_level\": noise_level,\n",
    "    \"domain_specific_score\": domain_specific_score,\n",
    "    \"relevance_score\": np.round(relevance_score, 3)\n",
    "})\n",
    "\n",
    "# Save CSV and ZIP it\n",
    "csv_name = \"context_relevance_dataset.csv\"\n",
    "zip_name = \"context_relevance_dataset.zip\"\n",
    "\n",
    "df.to_csv(csv_name, index=False)\n",
    "with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write(csv_name)\n",
    "\n",
    "print(f\"Dataset saved to {zip_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988a832b-0a0a-4e93-855f-00cdb72d38d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
